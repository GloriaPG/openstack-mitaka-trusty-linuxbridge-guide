Install Guide OpenStack Mitaka - GRE + Open vSwich

-- General --

1.- Configure Network Interfaces

Configure the first interface as the management interface:

IP address: 10.0.0.11

Network mask: 255.255.255.0 (or /24)

Default gateway: 10.0.0.1

2.- The provider interface uses a special configuration without an IP address assigned to it. Configure the second interface as the provider interface:

Replace INTERFACE_NAME with the actual interface name. For example, eth1 or ens224.

    Edit the /etc/network/interfaces file to contain the following:

    # The provider network interface
    auto INTERFACE_NAME
    iface INTERFACE_NAME inet manual
    up ip link set dev $IFACE up
    down ip link set dev $IFACE down

3.- Configure name resolution

Edit the /etc/hosts file to contain the following:

# controller
10.0.0.11       controller

# compute1
10.0.0.31       compute1

# block1
10.0.0.41       block1

# object1
10.0.0.51       object1

# object2
10.0.0.52       object2


Warning

Some distributions add an extraneous entry in the /etc/hosts file that resolves the actual hostname to another loopback IP address such as 127.0.1.1. You must comment out or remove this entry to prevent name resolution problems. Do not remove the 127.0.0.1 entry.

$ vi /etc/hosts

#127.0.1.1		controller

4.- Verify connectivity:

$ ping -c 4 openstack.org
PING openstack.org (162.242.140.107) 56(84) bytes of data.
64 bytes from 162.242.140.107: icmp_seq=1 ttl=50 time=131 ms
64 bytes from 162.242.140.107: icmp_seq=2 ttl=50 time=131 ms
64 bytes from 162.242.140.107: icmp_seq=3 ttl=50 time=131 ms
64 bytes from 162.242.140.107: icmp_seq=4 ttl=50 time=131 ms

--- openstack.org ping statistics ---
4 packets transmitted, 4 received, 0% packet loss, time 3004ms
rtt min/avg/max/mdev = 131.509/131.549/131.598/0.258 ms

5.- Configure NTP (Controller)

$ apt-get install chrony

6.- Edit the /etc/chrony/chrony.conf file and add, change, or remove the following keys as necessary for your environment:

server pool.ntp.org iburst

7.- Restart the NTP service:

$ # service chrony restart

8.- Verify NTP operation:

$ chronyc sources

9.- Enable the OpenStack repository

$ apt-get install software-properties-common

$ add-apt-repository cloud-archive:mitaka

10.- Finalize basic packages installation:

$ apt-get update && apt-get dist-upgrade

-- Controller --

11.- Install the OpenStack client:

$ apt-get install python-openstackclient

12.- Install OpenStack database (SQL) components:

$ apt-get install mariadb-server python-pymysql

13.- Create and edit the /etc/mysql/conf.d/openstack.cnf file and complete the following actions:

* In the [mysqld] section, set the bind-address key to the management IP address of the controller node to enable access by other nodes via the management network:

[mysqld]
bind-address = 10.0.0.11

* In the same [mysqld] section, set the following keys to enable useful options and the UTF-8 character set:

[mysqld]
...
default-storage-engine = innodb
innodb_file_per_table
max_connections = 4096
collation-server = utf8_general_ci
character-set-server = utf8

14.- Restart OpenStack database installation:

$ service mysql restart

15.- Secure database connection:

$ mysql_secure_installation

16.- Install OpenStack NoSQL database for telemetry:

$ apt-get install mongodb-server mongodb-clients python-pymongo

17.- Edit the /etc/mongodb.conf file and complete the following actions:

* Configure the bind_ip key to use the management interface IP address of the controller node.

bind_ip = 10.0.0.11

* By default, MongoDB creates several 1 GB journal files in the /var/lib/mongodb/journal directory. If you want to reduce the size of each journal file to 128 MB and limit total journal space consumption to 512 MB, assert the smallfiles key:

smallfiles = true

18.- Finalize NoSQL database installation:

$ service mongodb stop
$ rm /var/lib/mongodb/journal/prealloc.*
$ service mongodb start

19.- Install the Message Queue:

$ apt-get install rabbitmq-server

20.- Add the openstack user:

$ rabbitmqctl add_user openstack RABBIT_PASS
Creating user "openstack" ...
...done.

21.- Permit configuration, write, and read access for the openstack user:

$ rabbitmqctl set_permissions openstack ".*" ".*" ".*"
Setting permissions for user "openstack" in vhost "/" ...
...done.

22.- Install Memcache:

$ apt-get install memcached python-memcache

23.- Configure Memcache:

Edit the /etc/memcached.conf file and configure the service to use the management IP address of the controller node to enable access by other nodes via the management network:

-l 10.0.0.11

24.- Finalize Memcache installation:

$ service memcached restart

-- Keystone - Identity Service --

25.- Create Keystone database:

* Use the database access client to connect to the database server as the root user:

$ mysql -u root -p

* Create the keystone database:

> CREATE DATABASE keystone;

* Grant proper access to the keystone database:

> GRANT ALL PRIVILEGES ON keystone.* TO 'keystone'@'localhost' IDENTIFIED BY 'KEYSTONE_DBPASS';
> GRANT ALL PRIVILEGES ON keystone.* TO 'keystone'@'%' IDENTIFIED BY 'KEYSTONE_DBPASS';

* Exit the database access client:

> exit

26.- Generate a random value to use as the administration token during initial configuration:

$ openssl rand -hex 10
0538fbb20f023c045044

27.- Disable the keystone service from starting automatically after installation:

$ echo "manual" > /etc/init/keystone.override

28.- Install keystone packages:

$ apt-get install keystone apache2 libapache2-mod-wsgi

29.- Edit the /etc/keystone/keystone.conf file and complete the following actions:

* In the [DEFAULT] section, define the value of the initial administration token:

[DEFAULT]
...
admin_token = ADMIN_TOKEN

* In the [database] section, configure database access:

[database]
...
connection = mysql+pymysql://keystone:KEYSTONE_DBPASS@controller/keystone

* In the [token] section, configure the Fernet token provider:

[token]
...
provider = fernet

30.- Populate the Identity service database:

$ su -s /bin/sh -c "keystone-manage db_sync" keystone

31.- Initialize Fernet keys:

$ keystone-manage fernet_setup --keystone-user keystone --keystone-group keystone

32.- Configure the Apache HTTP Server by editing /etc/apache2/apache2.conf and configuring the ServerName option to reference the controller node:

ServerName controller

33.- Create the /etc/apache2/sites-available/wsgi-keystone.conf file with the following content:

Listen 5000
Listen 35357

<VirtualHost *:5000>
    WSGIDaemonProcess keystone-public processes=5 threads=1 user=keystone group=keystone display-name=%{GROUP}
    WSGIProcessGroup keystone-public
    WSGIScriptAlias / /usr/bin/keystone-wsgi-public
    WSGIApplicationGroup %{GLOBAL}
    WSGIPassAuthorization On
    ErrorLogFormat "%{cu}t %M"
    ErrorLog /var/log/apache2/keystone.log
    CustomLog /var/log/apache2/keystone_access.log combined

    <Directory /usr/bin>
        Require all granted
    </Directory>
</VirtualHost>

<VirtualHost *:35357>
    WSGIDaemonProcess keystone-admin processes=5 threads=1 user=keystone group=keystone display-name=%{GROUP}
    WSGIProcessGroup keystone-admin
    WSGIScriptAlias / /usr/bin/keystone-wsgi-admin
    WSGIApplicationGroup %{GLOBAL}
    WSGIPassAuthorization On
    ErrorLogFormat "%{cu}t %M"
    ErrorLog /var/log/apache2/keystone.log
    CustomLog /var/log/apache2/keystone_access.log combined

    <Directory /usr/bin>
        Require all granted
    </Directory>
</VirtualHost>

34.- Enable the Identity service virtual hosts:

$ ln -s /etc/apache2/sites-available/wsgi-keystone.conf /etc/apache2/sites-enabled

35.- Restart the Apache HTTP server:

$ service apache2 restart

36.- By default, the Ubuntu packages create an SQLite database, but this configuration uses an SQL database server, so you can remove the SQLite database file:

$ rm -f /var/lib/keystone/keystone.db

37.- Configure the authentication token:

$ export OS_TOKEN=ADMIN_TOKEN

38.- Configure the admin endpoint:

$ export OS_URL=http://controller:35357/v3

$ export OS_IDENTITY_API_VERSION=3

39.- Create the service entity and API endpoints:

$ openstack service create --name keystone --description "OpenStack Identity" identity

$ openstack endpoint create --region RegionOne identity public http://controller:5000/v3

$ openstack endpoint create --region RegionOne identity internal http://controller:5000/v3

$ openstack endpoint create --region RegionOne identity admin http://controller:35357/v3

40.- Create a domain, projects, users, and roles:

$ openstack domain create --description "Default Domain" default

$ openstack project create --domain default --description "Admin Project" admin

$ openstack user create --domain default --password-prompt admin

$ openstack role create admin

$ openstack role add --project admin --user admin admin

$ openstack project create --domain default --description "Service Project" service

$ openstack project create --domain default --description "Demo Project" demo

$ openstack user create --domain default --password-prompt demo

$ openstack role create user

$ openstack role add --project demo --user demo user

41.- Disable the temporary authentication token mechanism by editing /etc/keystone/keystone-paste.ini and remove admin_token_auth from [pipeline:public_api], [pipeline:admin_api], and [pipeline:api_v3] sections.

41.- Unset the temporary OS_TOKEN and OS_URL environment variables:

$ unset OS_TOKEN OS_URL

42.- As the admin user, request an authentication token:

$ openstack --os-auth-url http://controller:35357/v3 --os-project-domain-name default --os-user-domain-name default --os-project-name admin --os-username admin token issue

43.- As the demo user, request an authentication token:

$ openstack --os-auth-url http://controller:5000/v3 --os-project-domain-name default --os-user-domain-name default --os-project-name demo --os-username demo token issue

44.- Create the script admin-openrc and add the following content:

export OS_PROJECT_DOMAIN_NAME=default
export OS_USER_DOMAIN_NAME=default
export OS_PROJECT_NAME=admin
export OS_USERNAME=admin
export OS_PASSWORD=ADMIN_PASS
export OS_AUTH_URL=http://controller:35357/v3
export OS_IDENTITY_API_VERSION=3
export OS_IMAGE_API_VERSION=2

45.- Create the script demo-openrc and add the following content:

export OS_PROJECT_DOMAIN_NAME=default
export OS_USER_DOMAIN_NAME=default
export OS_PROJECT_NAME=demo
export OS_USERNAME=demo
export OS_PASSWORD=DEMO_PASS
export OS_AUTH_URL=http://controller:5000/v3
export OS_IDENTITY_API_VERSION=3
export OS_IMAGE_API_VERSION=2

46.- Load the admin-openrc file to populate environment variables with the location of the Identity service and the admin project and user credentials:

$ . admin-openrc

47.- Request an authentication token:

$ openstack token issue

-- Glance - Image Service --

48.- Create the glance database:

$ mysql -u root -p

> CREATE DATABASE glance;

> GRANT ALL PRIVILEGES ON glance.* TO 'glance'@'localhost' IDENTIFIED BY 'GLANCE_DBPASS';

> GRANT ALL PRIVILEGES ON glance.* TO 'glance'@'%' IDENTIFIED BY 'GLANCE_DBPASS';

49.- Create the glance user in keystone and give it admin role:

$ openstack user create --domain default --password-prompt glance

$ openstack role add --project service --user glance admin

50.- Create the Glance service entity:

$ openstack service create --name glance --description "OpenStack Image" image

51.- Create the Image service API endpoints:

$ openstack endpoint create --region RegionOne image public http://controller:9292

$ openstack endpoint create --region RegionOne image internal http://controller:9292

$ openstack endpoint create --region RegionOne image admin http://controller:9292

52.- Install the glance packages:

$ apt-get install glance

53.- Edit the /etc/glance/glance-api.conf file and complete the following actions:

In the [database] section, configure database access, by commenting :

[database]
...
connection = mysql+pymysql://glance:GLANCE_DBPASS@controller/glance

In the [keystone_authtoken] and [paste_deploy] sections, configure Identity service access:

[keystone_authtoken]
...
auth_uri = http://controller:5000
auth_url = http://controller:35357
memcached_servers = controller:11211
auth_type = password
project_domain_name = default
user_domain_name = default
project_name = service
username = glance
password = GLANCE_PASS

[paste_deploy]
...
flavor = keystone

In the [glance_store] section, configure the local file system store and location of image files:

[glance_store]
...
stores = file,http
default_store = file
filesystem_store_datadir = /var/lib/glance/images/

Edit the /etc/glance/glance-registry.conf file and complete the following actions:

[database]
...
connection = mysql+pymysql://glance:GLANCE_DBPASS@controller/glance

In the [keystone_authtoken] and [paste_deploy] sections, configure Identity service access:

[keystone_authtoken]
...
auth_uri = http://controller:5000
auth_url = http://controller:35357
memcached_servers = controller:11211
auth_type = password
project_domain_name = default
user_domain_name = default
project_name = service
username = glance
password = GLANCE_PASS

[paste_deploy]
...
flavor = keystone

54.- Populate the Image service database:

$ su -s /bin/sh -c "glance-manage db_sync" glance

55.- Restart the Image services:

$ service glance-registry restart

$ service glance-api restart

56.- Test image creation:

$ wget http://download.cirros-cloud.net/0.3.4/cirros-0.3.4-x86_64-disk.img

$ openstack image create "cirros" --file cirros-0.3.4-x86_64-disk.img --disk-format qcow2 --container-format bare --public

$ openstack image list

-- Nova - Compute Service (Controller Node) --

57.- Create the nova database:

$ mysql -u root -p

> CREATE DATABASE nova_api;

> CREATE DATABASE nova;

> GRANT ALL PRIVILEGES ON nova_api.* TO 'nova'@'localhost' IDENTIFIED BY 'NOVA_DBPASS';

> GRANT ALL PRIVILEGES ON nova_api.* TO 'nova'@'%' IDENTIFIED BY 'NOVA_DBPASS';

> GRANT ALL PRIVILEGES ON nova.* TO 'nova'@'localhost' IDENTIFIED BY 'NOVA_DBPASS';

> GRANT ALL PRIVILEGES ON nova.* TO 'nova'@'%' IDENTIFIED BY 'NOVA_DBPASS';

58.- Create the service credentials:

$ openstack user create --domain default --password-prompt nova

59.- Add the admin role to the nova user:

$ openstack role add --project service --user nova admin

60.- Create the nova service entity:

$ openstack service create --name nova --description "OpenStack Compute" compute

61.- Create the Compute service API endpoints:

$ openstack endpoint create --region RegionOne compute public http://controller:8774/v2.1/%\(tenant_id\)s

$ openstack endpoint create --region RegionOne compute internal http://controller:8774/v2.1/%\(tenant_id\)s

$ openstack endpoint create --region RegionOne compute admin http://controller:8774/v2.1/%\(tenant_id\)s

62.- Install the packages:

$ apt-get install nova-api nova-conductor nova-consoleauth nova-novncproxy nova-scheduler

63.- Edit the /etc/nova/nova.conf file and complete the following actions:

In the [DEFAULT] section, enable only the compute and metadata APIs:

[DEFAULT]
...
enabled_apis = osapi_compute,metadata

In the [api_database] and [database] sections, configure database access:

[api_database]
...
connection = mysql+pymysql://nova:NOVA_DBPASS@controller/nova_api

[database]
...
connection = mysql+pymysql://nova:NOVA_DBPASS@controller/nova

In the [DEFAULT] and [oslo_messaging_rabbit] sections, configure RabbitMQ message queue access:

[DEFAULT]
...
rpc_backend = rabbit

[oslo_messaging_rabbit]
...
rabbit_host = controller
rabbit_userid = openstack
rabbit_password = RABBIT_PASS

In the [DEFAULT] and [keystone_authtoken] sections, configure Identity service access:

[DEFAULT]
...
auth_strategy = keystone

[keystone_authtoken]
...
auth_uri = http://controller:5000
auth_url = http://controller:35357
memcached_servers = controller:11211
auth_type = password
project_domain_name = default
user_domain_name = default
project_name = service
username = nova
password = NOVA_PASS

In the [DEFAULT] section, configure the my_ip option to use the management interface IP address of the controller node:

[DEFAULT]
...
my_ip = 10.0.0.11

In the [DEFAULT] section, enable support for the Networking service:

[DEFAULT]
...
use_neutron = True
firewall_driver = nova.virt.firewall.NoopFirewallDriver

In the [vnc] section, configure the VNC proxy to use the management interface IP address of the controller node:

[vnc]
...
vncserver_listen = $my_ip
vncserver_proxyclient_address = $my_ip

In the [glance] section, configure the location of the Image service API:

[glance]
...
api_servers = http://controller:9292

In the [oslo_concurrency] section, configure the lock path:

[oslo_concurrency]
...
lock_path = /var/lib/nova/tmp

64.- Populate the Compute databases:

$ su -s /bin/sh -c "nova-manage api_db sync" nova

$ su -s /bin/sh -c "nova-manage db sync" nova

65.- Restart the Compute services:

$ service nova-api restart

$ service nova-consoleauth restart

$ service nova-scheduler restart

$ service nova-conductor restart

$ service nova-novncproxy restart

-- Nova - Compute Service (Compute Node) --

66.- Install the packages:

$ apt-get install nova-compute

67.- Edit the /etc/nova/nova.conf file and complete the following actions:

In the [DEFAULT] and [oslo_messaging_rabbit] sections, configure RabbitMQ message queue access:

[DEFAULT]
...
auth_strategy = keystone

[keystone_authtoken]
...
auth_uri = http://controller:5000
auth_url = http://controller:35357
memcached_servers = controller:11211
auth_type = password
project_domain_name = default
user_domain_name = default
project_name = service
username = nova
password = NOVA_PASS

In the [DEFAULT] section, configure the my_ip option:

[DEFAULT]
...
my_ip = MANAGEMENT_INTERFACE_IP_ADDRESS

In the [DEFAULT] section, enable support for the Networking service:

[DEFAULT]
...
use_neutron = True
firewall_driver = nova.virt.firewall.NoopFirewallDriver

In the [vnc] section, enable and configure remote console access:

[vnc]
...
enabled = True
vncserver_listen = 0.0.0.0
vncserver_proxyclient_address = $my_ip
novncproxy_base_url = http://controller:6080/vnc_auto.html

68.- Determine whether your compute node supports hardware acceleration for virtual machines:

If this command returns a value of one or greater, your compute node supports hardware acceleration which typically requires no additional configuration.

If this command returns a value of zero, your compute node does not support hardware acceleration and you must configure libvirt to use QEMU instead of KVM.

Edit the [libvirt] section in the /etc/nova/nova-compute.conf file as follows:

[libvirt]
...
virt_type = qemu

69.- Restart the Compute service:

$ service nova-compute restart

70.- List service components to verify successful launch and registration of each process:

$ openstack compute service list

-- Neutron - Networking Service (Controller Node) --

71.- Create the database:

$ mysql -u root -p

> CREATE DATABASE neutron;

> GRANT ALL PRIVILEGES ON neutron.* TO 'neutron'@'localhost' IDENTIFIED BY 'NEUTRON_DBPASS';

> GRANT ALL PRIVILEGES ON neutron.* TO 'neutron'@'%' IDENTIFIED BY 'NEUTRON_DBPASS';

72.- Create the neutron user:

$ openstack user create --domain default --password-prompt neutron

73.- Add the admin role to the neutron user:

$ openstack role add --project service --user neutron admin

74.- Create the neutron service entity:

$ openstack service create --name neutron --description "OpenStack Networking" network

75.- Create the Networking service API endpoints:

$ openstack endpoint create --region RegionOne network public http://controller:9696
  
$ openstack endpoint create --region RegionOne network internal http://controller:9696
  
$ openstack endpoint create --region RegionOne network admin http://controller:9696

76.- Install the components:

$ apt-get install neutron-server neutron-plugin-ml2 neutron-linuxbridge-agent neutron-l3-agent neutron-dhcp-agent neutron-metadata-agent

77.- Edit the /etc/neutron/neutron.conf file and complete the following actions:

In the [database] section, configure database access:

[database]
...
connection = mysql+pymysql://neutron:NEUTRON_DBPASS@controller/neutron

In the [DEFAULT] section, enable the Modular Layer 2 (ML2) plug-in, router service, and overlapping IP addresses:

[DEFAULT]
...
core_plugin = ml2
service_plugins = router
allow_overlapping_ips = True

In the [DEFAULT] and [oslo_messaging_rabbit] sections, configure RabbitMQ message queue access:

[DEFAULT]
...
rpc_backend = rabbit

[oslo_messaging_rabbit]
...
rabbit_host = controller
rabbit_userid = openstack
rabbit_password = RABBIT_PASS

In the [DEFAULT] and [keystone_authtoken] sections, configure Identity service access:

[DEFAULT]
...
auth_strategy = keystone

[keystone_authtoken]
...
auth_uri = http://controller:5000
auth_url = http://controller:35357
memcached_servers = controller:11211
auth_type = password
project_domain_name = default
user_domain_name = default
project_name = service
username = neutron
password = NEUTRON_PASS

In the [DEFAULT] and [nova] sections, configure Networking to notify Compute of network topology changes:

[DEFAULT]
...
notify_nova_on_port_status_changes = True
notify_nova_on_port_data_changes = True

[nova]
...
auth_url = http://controller:35357
auth_type = password
project_domain_name = default
user_domain_name = default
region_name = RegionOne
project_name = service
username = nova
password = NOVA_PASS

-- Neutron - ML2 Plugin (Controller Node) --

78.- Edit the /etc/neutron/plugins/ml2/ml2_conf.ini file and complete the following actions:

In the [ml2] section, enable flat, VLAN, and VXLAN networks:

[ml2]
...
type_drivers = flat,vlan,vxlan

In the [ml2] section, enable VXLAN self-service networks:

[ml2]
...
tenant_network_types = vxlan

[ml2]
...
mechanism_drivers = linuxbridge,l2population

In the [ml2] section, enable the port security extension driver:

[ml2]
...
extension_drivers = port_security

In the [ml2_type_flat] section, configure the provider virtual network as a flat network:

[ml2_type_flat]
...
flat_networks = provider

In the [ml2_type_vxlan] section, configure the VXLAN network identifier range for self-service networks:

[ml2_type_vxlan]
...
vni_ranges = 1:1000

In the [securitygroup] section, enable ipset to increase efficiency of security group rules:

[securitygroup]
...
enable_ipset = True

-- Neutron - Linux Bridge Agent (Controller Node) --

79.- Edit the /etc/neutron/plugins/ml2/linuxbridge_agent.ini file and complete the following actions:

In the [linux_bridge] section, map the provider virtual network to the provider physical network interface:

[linux_bridge]
physical_interface_mappings = provider:PROVIDER_INTERFACE_NAME

In the [vxlan] section, enable VXLAN overlay networks, configure the IP address of the physical network interface that handles overlay networks, and enable layer-2 population:

[vxlan]
enable_vxlan = True
local_ip = OVERLAY_INTERFACE_IP_ADDRESS
l2_population = True

In the [securitygroup] section, enable security groups and configure the Linux bridge iptables firewall driver:

[securitygroup]
...
enable_security_group = True
firewall_driver = neutron.agent.linux.iptables_firewall.IptablesFirewallDriver

-- Neutron - L3 Agent (Controller Node) --

80.- Edit the /etc/neutron/l3_agent.ini file and complete the following actions:

In the [DEFAULT] section, configure the Linux bridge interface driver and external network bridge:

[DEFAULT]
...
interface_driver = neutron.agent.linux.interface.BridgeInterfaceDriver
external_network_bridge =

-- Neutron - DHCP Agent (Controller Node) --

81.- Edit the /etc/neutron/dhcp_agent.ini file and complete the following actions:

In the [DEFAULT] section, configure the Linux bridge interface driver, Dnsmasq DHCP driver, and enable isolated metadata so instances on provider networks can access metadata over the network:

[DEFAULT]
...
interface_driver = neutron.agent.linux.interface.BridgeInterfaceDriver
dhcp_driver = neutron.agent.linux.dhcp.Dnsmasq
enable_isolated_metadata = True

-- Neutron - Networking Service (Compute Node) --

82.- Install the networking components on the compute node:

$ apt-get install neutron-linuxbridge-agent

83.- Edit the /etc/neutron/neutron.conf file and complete the following actions:

In the [DEFAULT] and [oslo_messaging_rabbit] sections, configure RabbitMQ message queue access:

[DEFAULT]
...
rpc_backend = rabbit

[oslo_messaging_rabbit]
...
rabbit_host = controller
rabbit_userid = openstack
rabbit_password = RABBIT_PASS

84.- In the [DEFAULT] and [keystone_authtoken] sections, configure Identity service access:

[DEFAULT]
...
auth_strategy = keystone

[keystone_authtoken]
...
auth_uri = http://controller:5000
auth_url = http://controller:35357
memcached_servers = controller:11211
auth_type = password
project_domain_name = default
user_domain_name = default
project_name = service
username = neutron
password = NEUTRON_PASS

-- Neutron - Linux Bridge Agent (Compute Node) --

85.- Edit the /etc/neutron/plugins/ml2/linuxbridge_agent.ini file and complete the following actions:

In the [linux_bridge] section, map the provider virtual network to the provider physical network interface:

[linux_bridge]
physical_interface_mappings = provider:PROVIDER_INTERFACE_NAME

In the [vxlan] section, enable VXLAN overlay networks, configure the IP address of the physical network interface that handles overlay networks, and enable layer-2 population:

[vxlan]
enable_vxlan = True
local_ip = OVERLAY_INTERFACE_IP_ADDRESS
l2_population = True

In the [securitygroup] section, enable security groups and configure the Linux bridge iptables firewall driver:

[securitygroup]
...
enable_security_group = True
firewall_driver = neutron.agent.linux.iptables_firewall.IptablesFirewallDriver

86.- Edit the /etc/nova/nova.conf file and complete the following actions:

In the [neutron] section, configure access parameters:

[neutron]
...
url = http://controller:9696
auth_url = http://controller:35357
auth_type = password
project_domain_name = default
user_domain_name = default
region_name = RegionOne
project_name = service
username = neutron
password = NEUTRON_PASS

87.- Restart the Compute service:

$ service nova-compute restart

88.- Restart the Linux bridge agent:

$ service neutron-linuxbridge-agent restart

-- Neutron - Metadata Agent (Controller Node) --

89.- Edit the /etc/neutron/metadata_agent.ini file and complete the following actions:

In the [DEFAULT] section, configure the metadata host and shared secret:

[DEFAULT]
...
nova_metadata_ip = controller
metadata_proxy_shared_secret = METADATA_SECRET

-- Neutron - Metadata Agent (Compute Node) --

90.- Configure Metadata at Compute node:

Edit the /etc/nova/nova.conf file and perform the following actions:

In the [neutron] section, configure access parameters, enable the metadata proxy, and configure the secret:

[neutron]
...
service_metadata_proxy = True
metadata_proxy_shared_secret = METADATA_SECRET

91.- Populate the database:

$ su -s /bin/sh -c "neutron-db-manage --config-file /etc/neutron/neutron.conf --config-file /etc/neutron/plugins/ml2/ml2_conf.ini upgrade head" neutron

92.- Restart the Compute API service:

$ service nova-api restart

93.- Restart the Networking services:

$ service neutron-server restart

$ service neutron-linuxbridge-agent restart

$ service neutron-dhcp-agent restart

$ service neutron-metadata-agent restart

$ service neutron-l3-agent restart

94.- List loaded extensions to verify successful launch of the neutron-server process:

$ neutron ext-list

-- Horizon - Dashboard (Controller Node) --

95.- Install the packages:

$ apt-get install openstack-dashboard

96.- Edit the /etc/openstack-dashboard/local_settings.py file and complete the following actions:

Configure the dashboard to use OpenStack services on the controller node:

OPENSTACK_HOST = "controller"

Allow all hosts to access the dashboard:

ALLOWED_HOSTS = ['*', ]

Configure the memcached session storage service:

SESSION_ENGINE = 'django.contrib.sessions.backends.cache'

CACHES = {
    'default': {
         'BACKEND': 'django.core.cache.backends.memcached.MemcachedCache',
         'LOCATION': 'controller:11211',
    }
}

Enable the Identity API version 3:

OPENSTACK_KEYSTONE_URL = "http://%s:5000/v3" % OPENSTACK_HOST

Enable support for domains:

OPENSTACK_KEYSTONE_MULTIDOMAIN_SUPPORT = True

Configure API versions:

OPENSTACK_API_VERSIONS = {
    "identity": 3,
    "image": 2,
    "volume": 2,
}

Configure default as the default domain for users that you create via the dashboard:

OPENSTACK_KEYSTONE_DEFAULT_DOMAIN = "default"

Configure user as the default role for users that you create via the dashboard:

OPENSTACK_KEYSTONE_DEFAULT_ROLE = "user"

Configure the time zone:

TIME_ZONE = "America/Chicago"

97.- Reload the web server configuration:

$ service apache2 reload

-- Cinder - Block Storage Service (Controller Node) --

98.- Create the database by completing these steps:

Connect to the database:

$ mysql -u root -p

Create the cinder database:

> CREATE DATABASE cinder;

Grant proper access to the cinder database:

> GRANT ALL PRIVILEGES ON cinder.* TO 'cinder'@'localhost' IDENTIFIED BY 'CINDER_DBPASS';

> GRANT ALL PRIVILEGES ON cinder.* TO 'cinder'@'%' IDENTIFIED BY 'CINDER_DBPASS';

99.- Create the service credentials:

$ openstack user create --domain default --password-prompt cinder

$ openstack role add --project service --user cinder admin

100.- Create the cinder and cinderv2 service entities:

$ openstack service create --name cinder --description "OpenStack Block Storage" volume

$ openstack service create --name cinderv2 --description "OpenStack Block Storage" volumev2

101.- Create the Block Storage service API endpoints:

$ openstack endpoint create --region RegionOne volume public http://controller:8776/v1/%\(tenant_id\)s
  
$ openstack endpoint create --region RegionOne volume internal http://controller:8776/v1/%\(tenant_id\)s
  
$ openstack endpoint create --region RegionOne volume admin http://controller:8776/v1/%\(tenant_id\)s
  
$ openstack endpoint create --region RegionOne volumev2 public http://controller:8776/v2/%\(tenant_id\)s
  
$ openstack endpoint create --region RegionOne volumev2 internal http://controller:8776/v2/%\(tenant_id\)s
  
$ openstack endpoint create --region RegionOne volumev2 admin http://controller:8776/v2/%\(tenant_id\)s

102.- Install the packages:

$ apt-get install cinder-api cinder-scheduler

103.- Edit the /etc/cinder/cinder.conf file and complete the following actions:

In the [DEFAULT] and [oslo_messaging_rabbit] sections, configure RabbitMQ message queue access:

[DEFAULT]
...
rpc_backend = rabbit

[oslo_messaging_rabbit]
...
rabbit_host = controller
rabbit_userid = openstack
rabbit_password = RABBIT_PASS

104.- In the [DEFAULT] and [keystone_authtoken] sections, configure Identity service access:

[DEFAULT]
...
auth_strategy = keystone

[keystone_authtoken]
...
auth_uri = http://controller:5000
auth_url = http://controller:35357
memcached_servers = controller:11211
auth_type = password
project_domain_name = default
user_domain_name = default
project_name = service
username = cinder
password = CINDER_PASS

105.- In the [DEFAULT] section, configure the my_ip option to use the management interface IP address of the controller node:

[DEFAULT]
...
my_ip = 10.0.0.11

106.- In the [oslo_concurrency] section, configure the lock path:

[oslo_concurrency]
...
lock_path = /var/lib/cinder/tmp

107.- Populate the Block Storage database:

$ su -s /bin/sh -c "cinder-manage db sync" cinder

-- Cinder - Block Storage Service (Compute Node) --

108.- Edit the /etc/nova/nova.conf file and add the following to it:

109.- Restart the Compute API service:

$ service nova-api restart

110.- Restart the Block Storage services:

$ service cinder-scheduler restart

$ service cinder-api restart

-- Cinder - Block Storage Service (Storage Node) --

111.- Install the supporting utility packages:

$ apt-get install lvm2

112.- Create the LVM physical volume (per example /dev/sdb):

$ pvcreate /dev/sdb

113.- Create the LVM volume group cinder-volumes:

$ vgcreate cinder-volumes /dev/sdb

114.- Edit the /etc/lvm/lvm.conf file and complete the following actions:

In the devices section, add a filter that accepts the /dev/sdb device and rejects all other devices:

devices {
...
filter = [ "a/sdb/", "r/.*/"]

115.- Install the packages:

$ apt-get install cinder-volume

116.- Edit the /etc/cinder/cinder.conf file and complete the following actions:

In the [database] section, configure database access:

In the [DEFAULT] and [oslo_messaging_rabbit] sections, configure RabbitMQ message queue access:

[DEFAULT]
...
rpc_backend = rabbit

[oslo_messaging_rabbit]
...
rabbit_host = controller
rabbit_userid = openstack
rabbit_password = RABBIT_PASS

In the [DEFAULT] and [keystone_authtoken] sections, configure Identity service access:

[DEFAULT]
...
auth_strategy = keystone

[keystone_authtoken]
...
auth_uri = http://controller:5000
auth_url = http://controller:35357
memcached_servers = controller:11211
auth_type = password
project_domain_name = default
user_domain_name = default
project_name = service
username = cinder
password = CINDER_PASS

In the [DEFAULT] section, configure the my_ip option:

[DEFAULT]
...
my_ip = MANAGEMENT_INTERFACE_IP_ADDRESS

In the [lvm] section, configure the LVM back end with the LVM driver, cinder-volumes volume group, iSCSI protocol, and appropriate iSCSI service:

[lvm]
...
volume_driver = cinder.volume.drivers.lvm.LVMVolumeDriver
volume_group = cinder-volumes
iscsi_protocol = iscsi
iscsi_helper = tgtadm

In the [DEFAULT] section, enable the LVM back end:

[DEFAULT]
...
enabled_backends = lvm

In the [DEFAULT] section, configure the location of the Image service API:

[DEFAULT]
...
glance_api_servers = http://controller:9292

In the [oslo_concurrency] section, configure the lock path:

[oslo_concurrency]
...
lock_path = /var/lib/cinder/tmp

117.- Restart the Block Storage volume service including its dependencies:

$ service tgt restart

$ service cinder-volume restart

118.- Verify operation of the Block Storage service:

$ cinder service-list

-- Heat - Orchestration Service --

119.- Create the database by completing these steps:

Use the database access client to connect to the database server as the root user:

$ mysql -u root -p

Create the heat database:

> CREATE DATABASE heat;

Grant proper access to the heat database:

> GRANT ALL PRIVILEGES ON heat.* TO 'heat'@'localhost' IDENTIFIED BY 'HEAT_DBPASS';

> GRANT ALL PRIVILEGES ON heat.* TO 'heat'@'%' IDENTIFIED BY 'HEAT_DBPASS';

120.- Create the heat user:

$ openstack user create --domain default --password-prompt heat

$ openstack role add --project service --user heat admin

121.- Create the heat and heat-cfn service entities:

$ openstack service create --name heat --description "Orchestration" orchestration

$ openstack service create --name heat-cfn --description "Orchestration" cloudformation

122.- Create the Orchestration service API endpoints:

$ openstack endpoint create --region RegionOne orchestration public http://controller:8004/v1/%\(tenant_id\)s

$ openstack endpoint create --region RegionOne orchestration internal http://controller:8004/v1/%\(tenant_id\)s

$ openstack endpoint create --region RegionOne orchestration admin http://controller:8004/v1/%\(tenant_id\)s

$ openstack endpoint create --region RegionOne cloudformation public http://controller:8000/v1
  
$ openstack endpoint create --region RegionOne cloudformation internal http://controller:8000/v1
  
$ openstack endpoint create --region RegionOne cloudformation admin http://controller:8000/v1

123.- Create the heat domain that contains projects and users for stacks:

$ openstack domain create --description "Stack projects and users" heat

124.- Create the heat_domain_admin user to manage projects and users in the heat domain:

$ openstack user create --domain heat --password-prompt heat_domain_admin

125.- Add the admin role to the heat_domain_admin user in the heat domain to enable administrative stack management privileges by the heat_domain_admin user:

$ openstack role add --domain heat --user-domain heat --user heat_domain_admin admin

126.- Create the heat_stack_owner role:

$ openstack role create heat_stack_owner

127.- Add the heat_stack_owner role to the demo project and user to enable stack management by the demo user:

$ openstack role add --project demo --user demo heat_stack_owner

128.- Create the heat_stack_user role:

$ openstack role create heat_stack_user

129.- Install the packages:

$ apt-get install heat-api heat-api-cfn heat-engine

130.- Edit the /etc/heat/heat.conf file and complete the following actions:

Edit the /etc/heat/heat.conf file and complete the following actions:

In the [database] section, configure database access:

[database]
...
connection = mysql+pymysql://heat:HEAT_DBPASS@controller/heat

In the [DEFAULT] and [oslo_messaging_rabbit] sections, configure RabbitMQ message queue access:

[DEFAULT]
...
rpc_backend = rabbit

[oslo_messaging_rabbit]
...
rabbit_host = controller
rabbit_userid = openstack
rabbit_password = RABBIT_PASS

In the [keystone_authtoken], [trustee], [clients_keystone], and [ec2authtoken] sections, configure Identity service access:

[keystone_authtoken]
...
auth_uri = http://controller:5000
auth_url = http://controller:35357
memcached_servers = controller:11211
auth_type = password
project_domain_name = default
user_domain_name = default
project_name = service
username = heat
password = HEAT_PASS

[trustee]
...
auth_plugin = password
auth_url = http://controller:35357
username = heat
password = HEAT_PASS
user_domain_name = default

[clients_keystone]
...
auth_uri = http://controller:35357

[ec2authtoken]
...
auth_uri = http://controller:5000

In the [DEFAULT] section, configure the metadata and wait condition URLs:

[DEFAULT]
...
heat_metadata_server_url = http://controller:8000
heat_waitcondition_server_url = http://controller:8000/v1/waitcondition

[DEFAULT]
...
stack_domain_admin = heat_domain_admin
stack_domain_admin_password = HEAT_DOMAIN_PASS
stack_user_domain_name = heat

131.- Populate the Orchestration database:

$ su -s /bin/sh -c "heat-manage db_sync" heat

132.- Restart the Orchestration services:

$ service heat-api restart

$ service heat-api-cfn restart

$ service heat-engine restart